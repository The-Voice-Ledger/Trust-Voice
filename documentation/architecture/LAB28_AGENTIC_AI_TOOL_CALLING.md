# Voice Ledger - Lab 28: Agentic AI â€” Tool-Calling Agent Architecture

**Comprehensive Educational Guide: From Rigid NLU Pipelines to Autonomous AI Agents**

---

## ğŸ“š What You'll Learn

By the end of this lab, you will be able to:

1. **Understand Agentic AI Architecture:**
   - OpenAI function-calling (tool-use) protocol
   - Why agents beat rigid intent-classification NLU pipelines
   - The agent loop: reason â†’ call tools â†’ observe â†’ respond
   - Multi-turn conversation via Redis-backed history
   - Read vs write tool classification for safety

2. **Build Production Agent Systems:**
   - Define OpenAI-compatible tool schemas (JSON function definitions)
   - Implement a tool registry that maps tool names â†’ handler functions
   - Build an agent executor with bounded turn limits
   - Handle bilingual input/output (Amharic â†” English translation)
   - Strip text for TTS (voice-safe output)

3. **Master Core Engineering Patterns:**
   - Singleton pattern for expensive resources (DB connections, blockchain clients)
   - Lazy imports for optional heavy dependencies
   - Graceful error handling with fallback cascades
   - Structured result objects with `@dataclass`
   - Comprehensive testing with mocked LLM calls

4. **Graduate as an AI Agent Engineer:**
   - Consulting-level understanding of agentic systems
   - Ability to extend any existing codebase with tool-calling agents
   - Deep knowledge of when agents help and when they hurt
   - Production hardening: safety rails, token budgets, timeout handling

---

## ğŸ¯ Lab Overview

**Duration:** 8-12 hours  
**Difficulty:** Intermediate to Advanced  
**Prerequisites:**
- Labs 1-8 completed (GS1, Voice, SSI, Blockchain, DPP, Docker, Telegram)
- Understanding of OpenAI API (chat completions)
- Python async/await and decorators
- Basic understanding of how Voice Ledger processes voice commands (Lab 7-8)

**What We're Building:**

A **tool-calling AI agent** that replaces Voice Ledger's rigid NLU â†’ switch/case pipeline with an autonomous reasoning loop. Instead of manually classifying user speech into 7 hardcoded intents, the agent chooses from **25 tools** and decides what arguments to pass â€” using GPT-4o function-calling.

```
OLD PIPELINE (Labs 7-8):
ğŸ¤ Voice â†’ STT â†’ NLU â†’ classify into 7 intents â†’ switch/case â†’ handler
                  â†“
           "record_commission"? â†’ execute_commission()
           "record_shipment"?  â†’ execute_shipment()
           "unknown"?          â†’ "Sorry, I don't understand"
           âš ï¸ Compound commands fail!
           âš ï¸ New intent = rewrite NLU prompt + add handler + add validation

NEW PIPELINE (Lab 28):
ğŸ¤ Voice â†’ STT â†’ Agent(tools=[25 tools]) â†’ reason â†’ call tools â†’ respond
                  â†“
           GPT-4o sees all 25 tools + user message
           Decides: "I need record_commission with qty=3000, origin=..."
           Calls tool â†’ gets result â†’ formats natural response
           âœ… Handles compound commands automatically
           âœ… New action = add one tool definition
           âœ… Asks for missing info naturally (no hand-coded questions)
```

**Real-World Context:**

Farmer Almaz in Yirgacheffe says: *"Ship batch BATCH-001 to Addis warehouse and also record 50 kilograms of new washed Arabica from my farm."*

**Old pipeline:** Fails. NLU classifies to one intent, loses the second command.

**New agent:** GPT-4o calls `record_shipment(batch_id="BATCH-001", destination="Addis warehouse")` **and** `record_commission(quantity_kg=50, origin="Almaz farm", variety="Washed Arabica")` in the same turn. Both succeed. Farmer gets one clean confirmation.

---

## ğŸ“– Theoretical Foundation: Why Agents?

Before writing code, let's understand why the entire industry shifted from NLU pipelines to agentic tool-calling in 2024-2025.

### The Old Way: Intent Classification

Voice Ledger Labs 7-8 built a classic NLU pipeline:

```
User: "I just harvested 50 bags of Sidama coffee from Gedeo farm"
           â†“
GPT-3.5 NLU Prompt:
  "Classify this into one of: record_commission, record_shipment,
   record_receipt, record_transformation, query_batches, unknown"
           â†“
Result: {intent: "record_commission", entities: {quantity: 50, ...}}
           â†“
switch (intent):
  case "record_commission": â†’ execute_commission(entities)
  case "record_shipment":   â†’ execute_shipment(entities)
  ...
  default: â†’ "I don't understand"
```

**What's wrong with this?**

| Problem | Impact | Example |
|---------|--------|---------|
| **Fixed intent list** | Can't handle new actions without rewriting NLU prompt | Adding marketplace = rewrite classifier |
| **Single intent per message** | Compound commands fail | "Ship X and record Y" â†’ picks one |
| **Manual entity extraction** | Brittle JSON parsing, misses edge cases | "3 bags" â†’ is that 3 kg or 180 kg? |
| **Hand-coded clarification** | Rigid question trees for missing info | If no quantity â†’ always ask same question |
| **No memory** | Each message is independent | "Ship it" â†’ "Ship what?" |

### The New Way: Tool-Calling Agents

OpenAI introduced function-calling in June 2023. By 2024, it became the standard pattern for building AI-powered applications:

```
User: "Ship batch BATCH-001 to Addis warehouse and record 50 bags new Sidama"
           â†“
GPT-4o receives:
  - System prompt: "You are Voice Ledger..."
  - Conversation history (from Redis)
  - User message
  - 25 tool definitions (JSON schemas)
           â†“
GPT-4o decides (internally):
  "The user wants TWO things:
   1. Ship BATCH-001 â†’ I need record_shipment
   2. New batch 50 bags (= 3000kg) Sidama â†’ I need record_commission"
           â†“
Returns tool_calls:
  [
    {name: "record_shipment", args: {batch_id: "BATCH-001", destination: "Addis warehouse"}},
    {name: "record_commission", args: {quantity_kg: 3000, origin: "farmer's farm", variety: "Sidama"}}
  ]
           â†“
System executes both â†’ feeds results back â†’ GPT-4o writes response:
  "âœ… Batch BATCH-001 shipped to Addis warehouse.
   âœ… New batch created: 3000 kg Sidama from your farm."
```

**Why is this better?**

| Capability | Old NLU | Agent | Winner |
|------------|---------|-------|--------|
| Multiple actions per message | âŒ Single intent | âœ… Multiple tool calls | Agent |
| Adding new actions | Rewrite NLU prompt + handler | Add tool definition | Agent |
| Asking for missing info | Hand-coded question trees | Natural conversation | Agent |
| Multi-turn memory | Separate state machine | Conversation history | Agent |
| Error recovery | Rigid retry logic | Natural re-ask | Agent |
| Language support | Per-intent prompts | One system prompt | Agent |
| Compound reasoning | Not possible | âœ… Chain tools together | Agent |

**Key Insight:** The agent replaces ~500 lines of brittle NLU + intent-matching + clarification code with a single reasoning loop that handles any combination of 25 tools.

### Architecture: The Agent Loop

The core pattern has three components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TOOLS (tools.py)                                    â”‚
â”‚     25 OpenAI function-calling schemas                  â”‚
â”‚     Each tool: name, description, parameters (JSON)     â”‚
â”‚     The model sees these as its "capabilities"          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. REGISTRY (registry.py)                              â”‚
â”‚     Maps each tool name â†’ Python handler function       â”‚
â”‚     Handler: (db, args, user_id, user_did) â†’ (msg, data)â”‚
â”‚     Manages DB sessions, singletons, error handling     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. EXECUTOR (executor.py)                              â”‚
â”‚     The agent loop:                                     â”‚
â”‚     while turns < max:                                  â”‚
â”‚       response = GPT-4o(messages, tools)                â”‚
â”‚       if response.tool_calls:                           â”‚
â”‚         for tc in tool_calls:                           â”‚
â”‚           result = registry.execute(tc.name, tc.args)   â”‚
â”‚           messages.append(result)                       â”‚
â”‚         continue  â† loop back for more reasoning        â”‚
â”‚       else:                                             â”‚
â”‚         return response.text  â† done, reply to user     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Token Flow:**
```
Turn 1: System prompt + history + user message + 25 tool schemas
        â†’ GPT-4o returns: tool_calls=[record_shipment, record_commission]
Turn 2: All above + tool results appended
        â†’ GPT-4o returns: "âœ… Both operations completed..."
Total: 2 API calls, ~2000 tokens, ~3 seconds
```

### Design Decisions

#### Decision 1: Tool Schemas vs ReAct Prompting

**Problem:** How should the agent know about available actions?

| Approach | How It Works | Pros | Cons |
|----------|-------------|------|------|
| **OpenAI Function-Calling** âœ… | Structured JSON schemas passed to model | Type-safe, reliable argument parsing, native support | Vendor lock-in to OpenAI |
| **ReAct Prompting** | Text description of tools in system prompt | Provider-agnostic | Brittle parsing, hallucinated args |
| **LangChain Agents** | Framework abstraction over multiple providers | Multi-provider | Heavy dependency, abstraction leak |

**Decision: OpenAI Function-Calling (native)**

**Rationale:**
- **Reliability:** 95%+ structured output accuracy vs ~70% with text-based tool parsing
- **Type safety:** JSON Schema enforces parameter types at the API level
- **Simplicity:** No regex parsing of model output, no "Action: tool_name\nInput: ..." formatting
- **Cost:** Direct API usage avoids LangChain's token overhead (~15-20% savings)
- **Speed:** Function-calling is optimized in the model's decoding path

**Trade-off Accepted:** OpenAI vendor lock-in â†’ Acceptable because we already use OpenAI for ASR (Whisper) and RAG (GPT-4). The tool schemas are portable JSON â€” migrating to Anthropic or open-source function-calling models requires only changing the API call.

#### Decision 2: Flat Tool List vs Agent Hierarchy

**Problem:** With 25 tools, should we use a single agent or multiple specialized agents?

| Approach | How It Works | Pros | Cons |
|----------|-------------|------|------|
| **Flat List** âœ… | One agent sees all 25 tools | Simple, no routing overhead | Token cost for all schemas |
| **Router + Specialists** | Meta-agent routes to domain agents | Each agent has fewer tools | Two LLM calls minimum, routing errors |
| **Dynamic Tool Loading** | Load tools based on user role/context | Minimal tokens | Complex, may miss needed tools |

**Decision: Flat list of 25 tools in one agent**

**Rationale:**
- **25 tools â‰ˆ 3000 tokens:** Well within GPT-4o's 128K context window (costs ~$0.01/call)
- **No routing errors:** Router agents can misclassify domain (marketplace vs compliance) causing complete failure
- **Compound commands work:** Agent sees all tools, can combine across domains in one turn
- **Simplicity:** One system prompt, one model call path, one executor

**Cost Analysis:**
```python
# Tool schema overhead per request
tokens_per_tool_schema = 120   # Average (name + description + parameters)
num_tools = 25
schema_overhead = 120 * 25     # = 3,000 tokens

# GPT-4o pricing
input_cost_per_1k = 0.0025    # $2.50 / 1M tokens
overhead_cost = (3000 / 1000) * input_cost_per_1k  # = $0.0075

# With 1000 farmers Ã— 10 commands/day Ã— 30 days
monthly_overhead = 0.0075 * 1000 * 10 * 30  # = $2,250/month
# Acceptable for production (< $0.01 per farmer per day)
```

**Threshold:** We'd switch to a router + specialist pattern at ~100 tools, where schema overhead approaches 12K tokens/call.

#### Decision 3: Bilingual Strategy (Translate-Bracket-Translate)

**Problem:** GPT-4o's tool-calling is trained primarily on English. Amharic tool-calling accuracy drops significantly.

| Approach | How It Works | Accuracy | Latency |
|----------|-------------|----------|---------|
| **Amharic tools directly** | Send Amharic text + tools to GPT-4o | ~60% tool-calling accuracy | 1 API call |
| **Translate-Bracket-Translate** âœ… | Translate Amharicâ†’English, agent reasons in English, translate response back | ~95% accuracy | 3 API calls |
| **Amharic-tuned model** | Fine-tune model for Amharic tool-calling | ~85% (estimated) | 1 API call, $$$$ training |

**Decision: Translate-Bracket-Translate pattern**

```
Amharic voice: "50 áŠªáˆ áˆ²á‹³áˆ› á‰¡áŠ“ á‰°áˆ°á‰¥áˆµá‰§áˆ"
    â†“ Addis AI Translation API (am â†’ en)
English: "50 kg Sidama coffee has been collected"
    â†“ GPT-4o (reasons in English, calls tools)
English response: "âœ… Batch created: 50kg Sidama"
    â†“ Addis AI Translation API (en â†’ am)
Amharic: "âœ… á‰£á‰½ á‰°áˆáŒ¥áˆ¯áˆ: 50 áŠªáˆ áˆ²á‹³áˆ›"
```

**Rationale:**
- **Accuracy:** English tool-calling is ~95% vs ~60% for Amharic
- **Cost:** Two translation API calls (~$0.001 each) << one failed tool call + retry
- **Fallback:** If Addis AI is down â†’ GPT-4o-mini translates â†’ if that fails â†’ return original

**Translation Cascade:**
```
1. Addis AI Translation API  â†’ Best quality for Amharic
2. GPT-4o-mini translation   â†’ Good fallback (fast, cheap)
3. Return original text      â†’ Last resort (user sees untranslated)
```

#### Decision 4: Read vs Write Safety Classification

**Problem:** Voice commands can modify data (create batches, ship, transform). A misheard voice command should not accidentally delete or modify farmer data.

**Solution: Classify every tool as READ or WRITE**

```python
# READ-ONLY tools (14 tools) â€” execute immediately, no confirmation
READ_ONLY = {
    "query_batches", "search_knowledge",           # Core
    "browse_rfqs", "list_my_offers",               # Marketplace
    "check_eudr_compliance", "check_mass_balance",  # Compliance
    "get_dpp", "get_container_dpp",                 # DPP
    "trace_lineage", "validate_dpp",               # DPP
    "list_pending_verifications",                    # Verification
    "check_blockchain_anchor", "get_token_info",    # Blockchain
    "verify_batch_hash",                            # Blockchain
}

# WRITE tools (11 tools) â€” agent should confirm if details are ambiguous
WRITE = {
    "record_commission", "record_shipment",
    "record_receipt", "record_transformation",
    "pack_batches", "unpack_batches", "split_batch",
    "create_rfq", "submit_offer", "accept_offer",
    "verify_batch",
}
```

**Behavior:**
- Read tool succeeds â†’ `performed_write = False` â†’ history preserved for follow-ups
- Write tool succeeds â†’ `performed_write = True` â†’ history cleared (fresh start)

**Why clear history after writes?** Prevents stale context like "ship it" referring to a batch from a previous conversation that was already shipped.

---

## ğŸ¤” Why This Design? Agent vs NLU Trade-offs

### When Agents Are Better Than NLU

| Scenario | NLU Pipeline | Agent | Winner |
|----------|-------------|-------|--------|
| Fixed 5-7 intents, simple entities | Fast, deterministic | Overkill | **NLU** |
| 25+ actions, complex arguments | Brittle, error-prone | Natural | **Agent** |
| Compound commands ("do X and Y") | Fails (single intent) | Handles natively | **Agent** |
| Missing info ("ship batch...where?") | Hand-coded question tree | Natural follow-up | **Agent** |
| New action every month | Rewrite NLU prompt each time | Add one tool definition | **Agent** |
| Multi-language (Amharic + English) | Per-language NLU prompts | One prompt + translate | **Agent** |

**Key Insight:** Voice Ledger crossed the threshold when it grew from 7 intents to 25+ actions across 7 domains. At that scale, maintaining a hand-tuned NLU classifier becomes untenable.

### When Agents Are Worse

| Concern | Mitigation |
|---------|------------|
| **Cost:** Every call sends 25 tool schemas (~3K extra tokens) | Acceptable at $0.0075/call |
| **Latency:** Agent loop may take 2-3 API calls | Bounded to 6 turns max |
| **Determinism:** Same input may produce different tool calls | Temperature 0.2 (near-deterministic) |
| **Hallucination:** Agent may call tools with fabricated arguments | Handlers validate all inputs |
| **Vendor lock-in:** OpenAI function-calling protocol | Schemas are portable JSON |

### Cost Comparison

```python
# OLD PIPELINE (Labs 7-8)
old_cost_per_command = (
    0.006 * 0.5    # Whisper ASR: 30 seconds = $0.003
    + 0.002        # GPT-3.5 NLU classification: ~500 tokens
    + 0.0          # Handler execution: free
)
# = $0.005 per command

# NEW AGENT (Lab 28)
new_cost_per_command = (
    0.006 * 0.5    # Whisper ASR: 30 seconds = $0.003
    + 0.01         # GPT-4o agent (2 turns avg, ~4K tokens): $0.01
    + 0.0          # Tool execution: free
)
# = $0.013 per command

# Monthly difference for 1000 farmers Ã— 10 commands/day
old_monthly = 0.005 * 1000 * 10 * 30  # = $1,500/month
new_monthly = 0.013 * 1000 * 10 * 30  # = $3,900/month
delta = new_monthly - old_monthly      # = $2,400/month (+160%)

# BUT: development time savings
# Old: 2 days to add new intent (NLU prompt + handler + validation + tests)
# New: 2 hours to add new tool (schema + handler + test)
# At 2 new features/month: saves ~28 dev hours/month
```

**Trade-off Accepted:** +$2,400/month API cost for 25x more capabilities + faster iteration. At scale (10K farmers), optimize with prompt caching, router patterns, or open-source models.

---

## ğŸ“‹ Prerequisites â€” What We Have (Labs 1-27)

### How Lab 28 Builds on Previous Labs

Lab 28 ties together **every module** built across the previous 27 labs. The agent is the new "brain" that orchestrates them all.

**From Labs 1-2 (GS1, Voice):**
```python
# Lab 28 WRAPS these in tool handlers:
from voice.command_integration import (
    execute_voice_command,   # â† Main command processor (Lab 7-8)
    process_commission,      # â† Create batch (Lab 1-2)
    process_shipment,        # â† Ship batch (Lab 1-2)
    process_receipt,         # â† Receive batch (Lab 1-2)
    process_transformation,  # â† Process coffee (Lab 1-2)
)
# Key Insight: Lab 28 doesn't rebuild these â€” it makes them callable by GPT-4o
```

**From Labs 3-4 (SSI, Blockchain):**
```python
# Lab 28 ADDS blockchain tools:
from blockchain.blockchain_anchor import BlockchainAnchor  # â† Check on-chain data
from blockchain.batch_hasher import hash_batch_from_db_model  # â† Verify integrity
from blockchain.token_manager import get_token_manager     # â† Token metadata

# Key Insight: Agent calls these as READ-ONLY verification tools
```

**From Lab 5 (DPP):**
```python
# Lab 28 WRAPS DPP builders as tools:
from dpp.dpp_builder import (
    build_dpp,             # â† Single batch passport
    build_aggregated_dpp,  # â† Container passport
    build_recursive_dpp,   # â† Full supply chain lineage
    validate_dpp,          # â† EUDR compliance check
)
# Key Insight: DPP functions manage their own DB sessions â€” agent's db is unused
```

**From Labs 7-8 (Voice, Telegram):**
```python
# Lab 28 REPLACES the NLU pipeline with an agent:
# OLD: transcript â†’ nlu_infer.extract_intent() â†’ switch/case
# NEW: transcript â†’ AgentExecutor.run() â†’ tool_calls â†’ response

# The Telegram bot calls the agent instead of the old command handler
from voice.agent import AgentExecutor
result = executor.run(transcript="50 bags Sidama", user_id=42)
```

**From Labs 14-15 (Marketplace):**
```python
# Lab 28 EXPOSES marketplace as 5 tools:
from database.crud import (
    create_rfq,          # â† Buyer creates RFQ
    get_active_rfqs,     # â† Browse marketplace
    submit_offer,        # â† Cooperative responds
    accept_offer,        # â† Buyer accepts
    get_offers_by_user,  # â† View my offers
)
# Key Insight: Agent enforces role-based access (only buyers can create RFQs)
```

**From Lab 16 (EUDR):**
```python
# Lab 28 WRAPS compliance checks as tools:
from voice.eudr.compliance import check_batch_eudr_status  # â† Deforestation check
# Mass balance validation is pure Python (no external dependency)
```

---

## ğŸ› ï¸ Step-by-Step Implementation

### Step 1: Project Structure

Create the agent module inside the `voice/` package:

```bash
# From the project root
mkdir -p voice/agent

# Create the 4 files we'll build
touch voice/agent/__init__.py
touch voice/agent/tools.py
touch voice/agent/registry.py
touch voice/agent/executor.py

# Verify structure
ls -la voice/agent/
```

**Expected Output:**
```
drwxr-xr-x  6 user  staff  192 Feb 13 10:00 .
drwxr-xr-x  12 user staff  384 Feb 13 10:00 ..
-rw-r--r--  1 user  staff    0 Feb 13 10:00 __init__.py
-rw-r--r--  1 user  staff    0 Feb 13 10:00 executor.py
-rw-r--r--  1 user  staff    0 Feb 13 10:00 registry.py
-rw-r--r--  1 user  staff    0 Feb 13 10:00 tools.py
```

**Module Purpose:**

| File | Lines | Purpose |
|------|-------|---------|
| `__init__.py` | ~30 | Public API: exports `AgentExecutor`, `AgentResult`, `SUPPLY_CHAIN_TOOLS` |
| `tools.py` | ~950 | 25 OpenAI function-calling schemas (JSON) |
| `registry.py` | ~1200 | Maps tool names â†’ handler functions (Python implementations) |
| `executor.py` | ~690 | The agent loop: GPT-4o â†’ tool calls â†’ results â†’ response |

**Total:** ~2,870 lines of agent code + ~1,010 lines of tests = **3,880 lines**

---

### Step 2: Install Dependencies

```bash
# Activate virtual environment
source venv/bin/activate

# Core dependencies (most already installed from previous labs)
pip install openai>=1.0          # OpenAI function-calling API
pip install httpx>=0.24          # Async HTTP for Addis AI translation
pip install redis>=4.0           # Conversation history storage

# Verify
python -c "import openai; print(f'OpenAI {openai.__version__}')"
python -c "import httpx; print(f'httpx {httpx.__version__}')"
python -c "import redis; print(f'redis {redis.__version__}')"
```

**Why these dependencies?**

| Package | Purpose | Why Not Alternatives |
|---------|---------|---------------------|
| `openai` | GPT-4o function-calling | Native SDK, best tool-calling support |
| `httpx` | Addis AI translation API calls | Async-capable, timeout support, modern `requests` replacement |
| `redis` | Conversation history storage | Already used for Celery (Lab 6), adds zero new infrastructure |

---

### Step 3: Build Tool Definitions (`tools.py`)

This is the **declarative layer** â€” we describe what each tool does using JSON schemas. The model reads these descriptions to decide which tools to call.

**Concept: OpenAI Function-Calling Schema**

Every tool is a JSON object with this structure:

```python
{
    "type": "function",
    "function": {
        "name": "tool_name",              # Unique identifier
        "description": "What this does",   # GPT reads this to decide when to use it
        "parameters": {
            "type": "object",
            "properties": {                # Arguments the tool accepts
                "arg_name": {
                    "type": "string",      # JSON Schema type
                    "description": "..."   # GPT reads this to extract from user speech
                }
            },
            "required": ["arg_name"]       # Which args are mandatory
        }
    }
}
```

**Key Insight:** The `description` field is the most important part. GPT-4o uses it to decide **when** to call the tool and **how** to fill in arguments from the user's natural language. A vague description = wrong tool calls. A precise description = reliable behavior.

#### Step 3.1: Core Supply Chain Tools (9 tools)

Create `voice/agent/tools.py`:

```python
"""
Agent Tool Definitions

Each tool is an OpenAI function-calling schema that maps to an existing
handler in voice/command_integration.py. The agent decides which tool(s)
to call based on the user's natural language â€” no manual intent classification.

Adding a new supply chain action = adding a new dict to SUPPLY_CHAIN_TOOLS.
"""

from typing import List, Dict, Any


# ---------------------------------------------------------------------------
# Tool: record_commission
# ---------------------------------------------------------------------------
RECORD_COMMISSION = {
    "type": "function",
    "function": {
        "name": "record_commission",
        "description": (
            "Create a NEW coffee batch. Use when a farmer reports a harvest, "
            "a new lot, or says they have coffee to register. "
            "Do NOT use if they reference an existing batch ID."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "quantity_kg": {
                    "type": "number",
                    "description": (
                        "Weight in kilograms. If user says bags, multiply by 60. "
                        "e.g. '50 bags' â†’ 3000"
                    ),
                },
                "origin": {
                    "type": "string",
                    "description": "Farm name, region, or location where coffee was produced",
                },
                "variety": {
                    "type": "string",
                    "description": (
                        "Coffee variety or product type. "
                        "e.g. Sidama, Yirgacheffe, Arabica, Washed Arabica"
                    ),
                },
                "grade": {
                    "type": "string",
                    "description": "Quality grade if mentioned (A, B, C, Grade 1, Grade 2)",
                    "default": "A",
                },
            },
            "required": ["quantity_kg", "origin"],
        },
    },
}
```

**Concept: Description Engineering**

Notice the `record_commission` description includes:
- **When to use:** "a farmer reports a harvest, a new lot"
- **When NOT to use:** "Do NOT use if they reference an existing batch ID"
- **Unit conversion:** "If user says bags, multiply by 60"

These instructions prevent the most common misclassifications. Without the negative instruction, GPT-4o would call `record_commission` when a farmer says "What about batch BATCH-001?" (referencing an existing batch, not creating one).

**Continue with the remaining tools.** Here is the pattern for each:

```python
# ---------------------------------------------------------------------------
# Tool: record_shipment
# ---------------------------------------------------------------------------
RECORD_SHIPMENT = {
    "type": "function",
    "function": {
        "name": "record_shipment",
        "description": (
            "Ship an EXISTING batch to a destination. Use when user says "
            "'ship', 'send', 'deliver', 'dispatch' and references a batch."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "batch_id": {
                    "type": "string",
                    "description": "Batch ID or GTIN of the batch to ship",
                },
                "destination": {
                    "type": "string",
                    "description": "Where the batch is being shipped to",
                },
            },
            "required": ["batch_id", "destination"],
        },
    },
}
```

**Exercise 3.1:** Following this pattern, define schemas for:
- `record_receipt` (receive a batch from someone)
- `record_transformation` (process coffee â€” roast, wash, dry)
- `pack_batches` (pack multiple batches into a shipping container)
- `unpack_batches` (unpack a container back into individual batches)
- `split_batch` (divide a batch into smaller portions)
- `query_batches` (look up batch data â€” READ ONLY)
- `search_knowledge` (search documentation â€” READ ONLY)

**Tip:** For each tool, think about:
1. What verbs would a farmer use? (Put those in the description)
2. What info is required vs optional? (Set `required` accordingly)
3. What units might confuse the model? (Add conversion notes in parameter descriptions)

#### Step 3.2: Marketplace Tools (5 tools)

```python
# ---------------------------------------------------------------------------
# Marketplace Agent (#3) â€” 5 tools
# ---------------------------------------------------------------------------
CREATE_RFQ = {
    "type": "function",
    "function": {
        "name": "create_rfq",
        "description": (
            "Create a new Request for Quote (RFQ) to buy coffee on the marketplace. "
            "Only buyers should use this. Creates a public listing that cooperatives "
            "can see and submit offers against."
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "variety": {
                    "type": "string",
                    "description": "Coffee variety wanted (e.g. Sidama, Yirgacheffe, Arabica)",
                },
                "quantity_kg": {
                    "type": "number",
                    "description": "Quantity wanted in kilograms",
                },
                "max_price_usd": {
                    "type": "number",
                    "description": "Maximum price per kg in USD (optional)",
                },
                "origin": {
                    "type": "string",
                    "description": "Preferred origin region (optional)",
                },
                "grade": {
                    "type": "string",
                    "description": "Minimum quality grade (optional)",
                },
            },
            "required": ["variety", "quantity_kg"],
        },
    },
}
```

**Key Insight:** Notice the description says "Only buyers should use this." The agent reads this and will ask non-buyer users to contact a buyer instead. Role-based access control through natural language descriptions.

**Exercise 3.2:** Define schemas for `browse_rfqs`, `submit_offer`, `accept_offer`, and `list_my_offers`.

#### Step 3.3: Compliance, DPP, Verification, and Blockchain Tools (11 tools)

Continue adding tools for the remaining 4 agent domains:

- **Compliance (2 tools):** `check_eudr_compliance`, `check_mass_balance`
- **DPP (4 tools):** `get_dpp`, `get_container_dpp`, `trace_lineage`, `validate_dpp`
- **Verification (2 tools):** `list_pending_verifications`, `verify_batch`
- **Blockchain (3 tools):** `check_blockchain_anchor`, `get_token_info`, `verify_batch_hash`

#### Step 3.4: Combine All Tools

At the bottom of `tools.py`, create the master list:

```python
# ---------------------------------------------------------------------------
# Master tool list â€” passed to GPT-4o as the 'tools' parameter
# ---------------------------------------------------------------------------
SUPPLY_CHAIN_TOOLS: List[Dict[str, Any]] = [
    # Core Supply Chain (9 tools)
    RECORD_COMMISSION,
    RECORD_SHIPMENT,
    RECORD_RECEIPT,
    RECORD_TRANSFORMATION,
    PACK_BATCHES,
    UNPACK_BATCHES,
    SPLIT_BATCH,
    QUERY_BATCHES,
    SEARCH_KNOWLEDGE,
    # Marketplace (5 tools)
    CREATE_RFQ,
    BROWSE_RFQS,
    SUBMIT_OFFER,
    ACCEPT_OFFER,
    LIST_MY_OFFERS,
    # Compliance (2 tools)
    CHECK_EUDR_COMPLIANCE,
    CHECK_MASS_BALANCE,
    # DPP / Traceability (4 tools)
    GET_DPP,
    GET_CONTAINER_DPP,
    TRACE_LINEAGE,
    VALIDATE_DPP,
    # Verification (2 tools)
    LIST_PENDING_VERIFICATIONS,
    VERIFY_BATCH,
    # Blockchain (3 tools)
    CHECK_BLOCKCHAIN_ANCHOR,
    GET_TOKEN_INFO,
    VERIFY_BATCH_HASH,
]
```

**Verification:**
```python
python -c "
from voice.agent.tools import SUPPLY_CHAIN_TOOLS
print(f'Total tools: {len(SUPPLY_CHAIN_TOOLS)}')
names = [t['function']['name'] for t in SUPPLY_CHAIN_TOOLS]
print(f'Unique names: {len(set(names))}')
assert len(names) == len(set(names)), 'Duplicate tool names!'
print('âœ… All tool names unique')
"
# Expected output:
# Total tools: 25
# Unique names: 25
# âœ… All tool names unique
```

---

### Step 4: Build the Tool Registry (`registry.py`)

The registry maps each tool name to a Python function that actually performs the operation. This is the **imperative layer** â€” where tools go from descriptions to actions.

**Concept: Registry Pattern**

A registry decouples **what** the agent can do (tool schemas in `tools.py`) from **how** it's done (handler functions in `registry.py`). This separation means:

- Tool descriptions can be tuned for GPT-4o without changing handler code
- Handlers can be swapped (mock for tests, real for production)
- New tools = add schema + add handler (no changes to executor)

```python
"""
Tool Registry

Maps each tool name to a handler function. Handlers are thin wrappers
around existing Voice Ledger modules â€” they adapt the module's interface
to the standard (db, args, user_id, user_did) â†’ (message, data) signature.
"""

import logging
from typing import Dict, Any, Tuple, Optional, Callable
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)


class ToolRegistry:
    """
    Maps tool names â†’ handler functions.

    Each handler has the signature:
        handler(db: Session, args: dict, user_id: int, user_did: str) -> (str, dict)

    Returns:
        tuple of (human-readable message, structured data dict)
    """

    def __init__(self):
        self._handlers: Dict[str, Callable] = {}
        self._register_all()

    def has(self, name: str) -> bool:
        return name in self._handlers

    def get(self, name: str) -> Optional[Callable]:
        return self._handlers.get(name)

    def register(self, name: str, handler: Callable):
        self._handlers[name] = handler

    # ------------------------------------------------------------------
    # Registration: bind all 25 tools to their handlers
    # ------------------------------------------------------------------
    def _register_all(self):
        self.register("record_commission", self._record_commission)
        self.register("record_shipment", self._record_shipment)
        # ... (register all 25 tools)
```

#### Step 4.1: Core Supply Chain Handlers

Each handler wraps an existing Voice Ledger function. Here's the pattern:

```python
    def _record_commission(
        self, db: Session, args: Dict[str, Any],
        user_id: int = None, user_did: str = None
    ) -> Tuple[str, Dict[str, Any]]:
        """Create a new coffee batch via voice command."""
        from voice.command_integration import process_commission

        # Extract and validate arguments
        quantity = args.get("quantity_kg")
        origin = args.get("origin", "Unknown")
        variety = args.get("variety", "Arabica")
        grade = args.get("grade", "A")

        if not quantity or quantity <= 0:
            return ("Please specify a valid quantity in kilograms.", {"error": "invalid_quantity"})

        # Call existing function (built in Lab 7-8)
        result = process_commission(
            db=db,
            user_id=user_id,
            user_did=user_did,
            quantity_kg=quantity,
            origin=origin,
            variety=variety,
            grade=grade,
        )

        batch = result.get("batch", {})
        batch_id = batch.get("batch_id", "Unknown")

        # Return human-readable message + structured data
        return (
            f"âœ… Batch {batch_id} created: {quantity} kg {variety} from {origin}",
            {
                "batch_id": batch_id,
                "quantity_kg": quantity,
                "origin": origin,
                "variety": variety,
            },
        )
```

**Concept: Handler Contract**

Every handler follows the same contract:

```python
def _handler_name(
    self,
    db: Session,           # Database session (managed by executor)
    args: Dict[str, Any],  # Arguments from GPT-4o's tool call
    user_id: int = None,   # Current user's database ID
    user_did: str = None   # Current user's DID (for credential operations)
) -> Tuple[str, Dict[str, Any]]:
    """
    Returns:
        tuple of:
        - str: Human-readable message (shown to user, spoken by TTS)
        - dict: Structured data (for Mini App display, logging, etc.)
    """
```

This uniform signature means the executor doesn't need to know anything about individual tools â€” it just calls `handler(db, args, user_id=..., user_did=...)` for every tool.

#### Step 4.2: Integration Pitfalls (What We Learned the Hard Way)

When building handlers that wrap existing modules, the **return structures of those modules** must be matched exactly. Here are real bugs we encountered and how to avoid them:

**Pitfall 1: Wrong Dictionary Keys**

```python
# âŒ WRONG â€” assumes the DPP builder returns 'name' for product
product_name = product.get('name', 'Coffee')

# âœ… CORRECT â€” actual key in dpp_builder.py is 'productName'
product_name = product.get('productName', 'Coffee')
```

**How to avoid:** Always read the actual source code of the module you're wrapping. Don't guess key names from documentation or memory. Run the function once and print the output to verify.

**Pitfall 2: Formatting Already-Formatted Strings**

```python
# âŒ WRONG â€” contributionPercent is already "33.3%" (a string)
contrib_lines.append(f"  â€¢ {name}: {pct:.1f}%")   # TypeError: unsupported format for str

# âœ… CORRECT â€” use the string directly
contrib_lines.append(f"  â€¢ {name}: {pct}")
```

**How to avoid:** Check the type of every value you read from another module's return. If it's already formatted, don't re-format it.

**Pitfall 3: Wrong Import Names**

```python
# âŒ WRONG â€” function doesn't exist with this name
from blockchain.batch_hasher import hash_batch_model  # ImportError!

# âœ… CORRECT â€” actual function name
from blockchain.batch_hasher import hash_batch_from_db_model
```

**How to avoid:** Use `grep` or your IDE's "Go to Definition" to verify the exact function name before importing it.

**Pitfall 4: Re-creating Expensive Objects**

```python
# âŒ WRONG â€” creates new Web3 connection on EVERY tool call (~200-500ms)
def _check_blockchain_anchor(self, db, args, **kwargs):
    from blockchain.blockchain_anchor import BlockchainAnchor
    anchor = BlockchainAnchor()  # Expensive! Web3 + RPC + ABI read + key derivation
    info = anchor.get_batch_info(batch_id)

# âœ… CORRECT â€” singleton pattern, created once, reused forever
_blockchain_anchor = None

def _get_blockchain_anchor():
    global _blockchain_anchor
    if _blockchain_anchor is None:
        from blockchain.blockchain_anchor import BlockchainAnchor
        _blockchain_anchor = BlockchainAnchor()
    return _blockchain_anchor

def _check_blockchain_anchor(self, db, args, **kwargs):
    anchor = _get_blockchain_anchor()  # 0ms on subsequent calls
    info = anchor.get_batch_info(batch_id)
```

**Concept: Singleton Pattern**

Some objects are expensive to create (database connections, blockchain clients, ML models). The singleton pattern ensures they're created once and shared:

```
First call:  _get_blockchain_anchor() â†’ create new â†’ ~300ms
Second call: _get_blockchain_anchor() â†’ return cached â†’ ~0ms
Third call:  _get_blockchain_anchor() â†’ return cached â†’ ~0ms
```

Voice Ledger uses singletons for:
- `get_tool_registry()` â€” the ToolRegistry instance
- `get_token_manager()` â€” blockchain token manager (from Lab 4)
- `_get_blockchain_anchor()` â€” blockchain anchor client (new in Lab 28)

#### Step 4.3: DPP Handlers (Careful Integration)

DPP handlers are unique because the DPP builder manages its **own database sessions**. The `db` parameter from the agent executor is unused:

```python
    def _get_dpp(
        self, db: Session, args: Dict[str, Any],
        user_id: int = None, user_did: str = None
    ) -> Tuple[str, Dict[str, Any]]:
        """Get the Digital Product Passport for a batch."""
        batch_id = args.get("batch_id")
        if not batch_id:
            return ("Please specify a batch ID.", {"error": "no_batch_id"})

        try:
            from dpp.dpp_builder import build_dpp
            dpp = build_dpp(batch_id=batch_id)  # Opens its own DB session!
        except ValueError as e:
            return (f"Batch not found: {e}", {"error": str(e)})

        # âš ï¸ Key names must match actual dpp_builder.py output
        product = dpp.get("productInformation", {})
        bc = dpp.get("blockchain", {})

        # Blockchain anchors are in a LIST, not at top level
        anchors = bc.get("anchors", [])
        is_anchored = any(a.get("transactionHash") for a in anchors) if anchors else False

        return (
            f"ğŸ“‹ DPP for {dpp.get('batchId', batch_id)}:\n"
            f"â€¢ Product: {product.get('productName', 'Coffee')}\n"  # NOT 'name'!
            f"â€¢ Blockchain: {'âœ… Anchored' if is_anchored else 'â³ Pending'}",
            {"batch_id": dpp.get("batchId"), "anchored": is_anchored},
        )
```

**Key Insight:** When wrapping a module that manages its own sessions, the agent's `db` parameter is benign (unused but harmless). Don't pass it to the wrapped function â€” let it open its own session.

#### Step 4.4: Blockchain Handlers (Singleton + Error Handling)

```python
    def _check_blockchain_anchor(
        self, db: Session, args: Dict[str, Any],
        user_id: int = None, user_did: str = None
    ) -> Tuple[str, Dict[str, Any]]:
        """Check if a batch is anchored on the blockchain."""
        batch_id = args.get("batch_id")
        if not batch_id:
            return ("Please specify a batch ID.", {"error": "no_batch_id"})

        try:
            anchor = _get_blockchain_anchor()  # Singleton!
            info = anchor.get_batch_info(batch_id)
        except Exception as e:
            logger.warning(f"Blockchain query failed for {batch_id}: {e}")
            return (
                f"Could not check blockchain for '{batch_id}'. "
                "The blockchain node may be unavailable.",
                {"error": str(e)},
            )

        if not info:
            return (
                f"Batch {batch_id} is not yet anchored on the blockchain.",
                {"batch_id": batch_id, "anchored": False},
            )

        from datetime import datetime
        ts = info.get("timestamp", 0)
        anchor_time = (
            datetime.utcfromtimestamp(ts).strftime("%Y-%m-%d %H:%M UTC")
            if ts else "Unknown"
        )

        return (
            f"âœ… Batch {batch_id} is anchored on Base Sepolia:\n"
            f"â€¢ Event type: {info.get('event_type', '?')}\n"
            f"â€¢ IPFS: {info.get('ipfs_cid', 'N/A')}\n"
            f"â€¢ Anchored: {anchor_time}",
            {"batch_id": batch_id, "anchored": True, "timestamp": ts},
        )
```

**Exercise 4.1:** Implement `_verify_batch_hash` following this pattern. Remember:
- Import `hash_batch_from_db_model` (not `hash_batch_model`!)
- Use `_get_blockchain_anchor()` singleton (not `BlockchainAnchor()`)
- Compare hashes after normalizing: lowercase, strip "0x" prefix
- Handle `None` from `get_batch_info()` (batch not anchored yet)

#### Step 4.5: Module-Level Singleton Factory

At the bottom of `registry.py`, add the singleton pattern:

```python
# Module-level singleton for the registry itself
_registry: Optional[ToolRegistry] = None


def get_tool_registry() -> ToolRegistry:
    """Get or create the global tool registry."""
    global _registry
    if _registry is None:
        _registry = ToolRegistry()
    return _registry
```

---

### Step 5: Build the Agent Executor (`executor.py`)

The executor is the **core agent loop**. It's ~690 lines and handles everything: translation, conversation history, the GPT-4o call loop, tool execution, and response formatting.

#### Step 5.1: Translation Layer

The translate-bracket-translate pattern for Amharic:

```python
def translate_text(text: str, source_lang: str, target_lang: str) -> str:
    """
    Translate text between English and Amharic.

    Strategy:
    1. Try Addis AI Translation API (best for Amharic)
    2. Fall back to GPT-4o-mini translation
    3. Return original text if both fail
    """
    if not text or not text.strip():
        return text

    # Try Addis AI first (preferred for Amharic quality)
    if ADDIS_AI_API_KEY:
        try:
            import httpx
            resp = httpx.post(
                ADDIS_TRANSLATE_URL,
                headers={
                    "X-API-Key": ADDIS_AI_API_KEY,
                    "Content-Type": "application/json",
                },
                json={
                    "text": text,
                    "source_language": source_lang,
                    "target_language": target_lang,
                },
                timeout=15.0,
            )
            resp.raise_for_status()
            data = resp.json()
            translated = (
                data.get("data", {}).get("translated_text")
                or data.get("translated_text")
                or data.get("translation")
            )
            if translated and translated.strip():
                return translated.strip()
        except Exception as e:
            logger.warning(f"Addis AI translation failed, trying GPT fallback: {e}")

    # Fallback: GPT-4o-mini translation
    try:
        resp = _client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"Translate to {target_lang}. Return ONLY the translation."},
                {"role": "user", "content": text},
            ],
            temperature=0.2,
            max_tokens=1000,
        )
        translated = resp.choices[0].message.content.strip()
        if translated:
            return translated
    except Exception as e:
        logger.warning(f"GPT translation also failed: {e}")

    return text  # Return original as last resort
```

**Concept: Fallback Cascade**

Production systems should never have a single point of failure. The translation layer uses a 3-level cascade:

```
Level 1: Addis AI Translation API
    â†“ (fails: network error, rate limit, API down)
Level 2: GPT-4o-mini translation
    â†“ (fails: OpenAI down, rate limit)
Level 3: Return original text untranslated
    â†’ User sees English, but system doesn't crash
```

This pattern â€” **primary â†’ fallback â†’ graceful degradation** â€” appears throughout Voice Ledger.

#### Step 5.2: System Prompt

The system prompt defines the agent's personality and rules:

```python
AGENT_SYSTEM_PROMPT = """You are Voice Ledger â€” an AI assistant for Ethiopian coffee supply chain actors.

YOUR CAPABILITIES (use the tools provided):
â€¢ Create new coffee batches (record_commission)
â€¢ Ship batches (record_shipment)
â€¢ Look up batches and data (query_batches)
[... all 25 tools listed by category ...]

CONVERSATION RULES:
1. Be warm, clear, and concise â€” users speak via voice
2. When a user gives all needed info, call the tool immediately
3. When info is missing, ask naturally â€” ONE question at a time
4. After executing a tool, summarize the result clearly
5. You can call MULTIPLE tools in one turn
6. For quantities in "bags", convert to kg (1 bag = 60 kg)

RESPONSE STYLE:
- Keep responses SHORT for voice â€” 2-3 sentences max
- Never mention technical internals (EPCIS, GS1) unless asked

SAFETY:
- For write operations, confirm if details seem ambiguous
- For read operations, execute immediately
- Never fabricate batch IDs or data
"""
```

**Design Principle: Prompt Engineering for Agents**

| Rule | Why |
|------|-----|
| "Be warm, clear, and concise" | Voice users can't re-read, must understand first time |
| "ONE question at a time" | Multiple questions confuse voice interaction |
| "MULTIPLE tools in one turn" | Enables compound commands |
| "1 bag = 60 kg" | Prevents unit confusion (most common agent error) |
| "Never mention EPCIS/GS1" | Farmers don't know these terms |
| "Confirm ambiguous writes" | Prevents accidental data modification |

#### Step 5.3: Conversation History (Redis-Backed)

Multi-turn conversations need memory. We store message history in Redis with a 10-minute TTL:

```python
def get_conversation_history(user_id: int, max_messages: int = 20) -> List[Dict]:
    """Retrieve conversation history from Redis."""
    r = _get_redis()
    if not r:
        return []  # Graceful degradation: no Redis = no history (still works)

    try:
        key = f"agent:history:{user_id}"
        data = r.get(key)
        if not data:
            return []
        messages = json.loads(data)
        return messages[-max_messages:]  # Only last N to stay within context window
    except Exception:
        return []


def save_conversation_history(user_id: int, messages: List[Dict], ttl: int = 600):
    """Save history to Redis with 10-minute TTL."""
    r = _get_redis()
    if not r:
        return

    try:
        key = f"agent:history:{user_id}"
        # Serialize tool_calls for storage (they contain non-JSON objects)
        serializable = []
        for msg in messages:
            entry = {"role": msg["role"]}
            if msg.get("content"):
                entry["content"] = msg["content"]
            if msg.get("tool_calls"):
                entry["tool_calls"] = [
                    {
                        "id": tc.id if hasattr(tc, "id") else tc.get("id"),
                        "type": "function",
                        "function": {
                            "name": tc.function.name if hasattr(tc, "function") else tc.get("function", {}).get("name"),
                            "arguments": tc.function.arguments if hasattr(tc, "function") else tc.get("function", {}).get("arguments", "{}"),
                        },
                    }
                    for tc in msg["tool_calls"]
                ]
            if msg.get("tool_call_id"):
                entry["tool_call_id"] = msg["tool_call_id"]
            serializable.append(entry)

        r.setex(key, ttl, json.dumps(serializable))
    except Exception as e:
        logger.warning(f"Failed to save history: {e}")
```

**Concept: Serializing Tool Calls**

OpenAI returns tool calls as Python objects with attributes (`tc.function.name`). But Redis stores strings. We must convert:

```python
# OpenAI returns objects:
tc.id               # "call_abc123"
tc.function.name     # "record_commission"
tc.function.arguments # '{"quantity_kg": 50}'

# Redis needs dicts:
{"id": "call_abc123", "function": {"name": "record_commission", "arguments": "..."}}
```

The `hasattr` check handles both fresh objects (from OpenAI) and restored dicts (from Redis).

#### Step 5.4: The Agent Loop

This is the heart of the system â€” the bounded reasoning loop:

```python
class AgentExecutor:
    def run(self, transcript: str, user_id: int, language: str = "en", ...) -> AgentResult:
        start_time = time.time()
        is_amharic = language == "am"

        # Step 1: Translate Amharic â†’ English (if needed)
        if is_amharic:
            transcript = translate_text(transcript, "am", "en")

        # Step 2: Build messages (system + history + user)
        messages = [{"role": "system", "content": self.system_prompt}]
        messages.extend(get_conversation_history(user_id))
        messages.append({"role": "user", "content": transcript})

        # Step 3: Agent loop (bounded to max_turns)
        for turn in range(self.max_turns):  # Default: 6 turns max

            # Call GPT-4o with tools
            response = _client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.tools,           # All 25 tool schemas
                tool_choice="auto",         # Let model decide
                temperature=0.2,            # Near-deterministic
            )

            msg = response.choices[0].message

            # Case 1: Model wants to call tools
            if msg.tool_calls:
                messages.append({"role": "assistant", "tool_calls": msg.tool_calls})

                for tc in msg.tool_calls:
                    # Execute the tool
                    result = self._execute_tool(tc.function.name, json.loads(tc.function.arguments))

                    # Append result as tool message
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": json.dumps(result),
                    })

                continue  # â† Loop back! Model may want to call more tools or respond

            # Case 2: Model returns text (done reasoning)
            response_text = msg.content

            # Translate response â†’ Amharic (if needed)
            if is_amharic:
                response_text = translate_text(response_text, "en", "am")

            # Save history, return result
            save_conversation_history(user_id, messages[1:])  # Skip system prompt
            return AgentResult(response=response_text, ...)

        # Exhausted max turns â€” safety exit
        return AgentResult(response="I'm having trouble. Could you try rephrasing?")
```

**Concept: Bounded Agent Loop**

The `for turn in range(self.max_turns)` prevents runaway agents:

```
Turn 1: GPT-4o â†’ calls record_commission â†’ result appended
Turn 2: GPT-4o â†’ calls record_shipment â†’ result appended
Turn 3: GPT-4o â†’ no more tools â†’ returns text response â† DONE

Maximum: 6 turns (configurable via AGENT_MAX_TURNS env var)
If exceeded: return error message (never infinite loop)
```

**Why max 6 turns?** Analysis of production Voice Ledger usage shows:
- 80% of commands resolve in 1 turn (single tool call)
- 15% need 2 turns (tool call + follow-up question)
- 4% need 3 turns (complex compound commands)
- <1% need 4+ turns (edge cases, very ambiguous input)

6 turns provides generous headroom while preventing runaway API costs.

#### Step 5.5: Tool Execution with Database Sessions

```python
    def _execute_tool(self, tool_name, args, user_id=None, user_did=None):
        """Execute a tool with proper DB session management."""
        handler = self.registry.get(tool_name)
        if not handler:
            return {"success": False, "message": f"Unknown tool: {tool_name}"}

        from database.connection import get_db

        try:
            with get_db() as db:  # Context manager: auto-commit or rollback
                message, data = handler(db, args, user_id=user_id, user_did=user_did)
                return {"success": True, "message": message, "data": data}
        except Exception as e:
            logger.error(f"Tool {tool_name} failed: {e}", exc_info=True)
            return {"success": False, "message": f"Operation failed: {str(e)}"}
```

**Concept: Context Manager for Database Sessions**

The `with get_db() as db:` pattern ensures:
- Session is created before the handler runs
- If handler succeeds â†’ session commits
- If handler raises â†’ session rolls back
- Session is always closed (even on crash)

This prevents database connection leaks â€” a common production failure mode.

#### Step 5.6: Speech Stripping for TTS

Voice responses shouldn't include URLs, markdown, or emoji:

```python
    @staticmethod
    def _strip_for_speech(text: str) -> str:
        """Strip URLs, emoji, and markdown for TTS output."""
        import re
        text = re.sub(r"https?://\S+", "", text)          # Remove URLs
        text = re.sub(r"\*+([^*]+)\*+", r"\1", text)      # Remove **bold**
        text = re.sub(r"^#+\s*", "", text, flags=re.MULTILINE)  # Remove # headers
        text = re.sub(r"^[â€¢\-]\s*", "", text, flags=re.MULTILINE)  # Remove bullets
        text = re.sub(r"[\U0001F300-\U0001F9FF]", "", text)  # Remove emoji
        text = re.sub(r"\s+", " ", text).strip()           # Collapse whitespace
        return text
```

**Why?** TTS engines will literally say "h-t-t-p-s colon slash slash..." or "asterisk asterisk" if you don't strip these.

---

### Step 6: Build the Package Init (`__init__.py`)

```python
"""
Voice Ledger AI Agent

Tool-calling agent that replaces the rigid NLU â†’ switch/case pipeline.
Instead of classifying into 7 hardcoded intents, the agent reasons about
which tools to call and what arguments to pass â€” using OpenAI function-calling.

Architecture:
    Voice â†’ STT â†’ Agent(tools=[...]) â†’ tool calls â†’ results â†’ response

Benefits over the old pipeline:
    - No brittle NLU system prompt with 7 intent definitions
    - Handles compound commands ("ship batch 001 AND record 50kg new batch")
    - Asks for missing info naturally (no hand-coded clarification questions)
    - New actions = new tool definitions (no rewriting NLU + handler + validation)
    - Multi-turn memory via conversation history (no separate state machine)
"""

from .executor import AgentExecutor, AgentResult
from .tools import SUPPLY_CHAIN_TOOLS
from .registry import ToolRegistry

__all__ = [
    "AgentExecutor",
    "AgentResult",
    "SUPPLY_CHAIN_TOOLS",
    "ToolRegistry",
]
```

**Concept: `__all__` and Public API**

`__all__` explicitly defines what `from voice.agent import *` exports. This:
- Documents the public API
- Prevents internal helpers from leaking
- Enables IDE auto-completion

---

### Step 7: Write Tests (`tests/test_agent.py`)

Comprehensive testing is critical for agent systems because LLM output is non-deterministic. We test everything **except** the LLM itself (which we mock).

#### Step 7.1: Test Tool Schemas

```python
import pytest
import json
from unittest.mock import patch, MagicMock


class TestToolDefinitions:
    """Verify all 25 tool schemas are valid OpenAI function-calling format."""

    def test_all_tools_have_valid_schema(self):
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS

        for tool in SUPPLY_CHAIN_TOOLS:
            assert tool["type"] == "function"
            fn = tool["function"]
            assert "name" in fn, "Tool missing 'name'"
            assert "description" in fn, f"Tool {fn.get('name')} missing 'description'"
            assert "parameters" in fn, f"Tool {fn.get('name')} missing 'parameters'"
            assert fn["parameters"]["type"] == "object"

    def test_total_tool_count(self):
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS
        assert len(SUPPLY_CHAIN_TOOLS) == 25

    def test_tool_names_are_unique(self):
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS
        names = [t["function"]["name"] for t in SUPPLY_CHAIN_TOOLS]
        assert len(names) == len(set(names)), f"Duplicate names: {names}"

    def test_required_fields_are_in_properties(self):
        """Every required field must also appear in properties."""
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS

        for tool in SUPPLY_CHAIN_TOOLS:
            fn = tool["function"]
            props = fn["parameters"].get("properties", {})
            required = fn["parameters"].get("required", [])
            for req in required:
                assert req in props, (
                    f"Tool '{fn['name']}': required field '{req}' not in properties"
                )
```

#### Step 7.2: Test Registry

```python
class TestToolRegistry:
    """Verify all tools have matching handlers."""

    def test_all_tools_registered(self):
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS
        from voice.agent.registry import ToolRegistry

        registry = ToolRegistry()
        for tool in SUPPLY_CHAIN_TOOLS:
            name = tool["function"]["name"]
            assert registry.has(name), f"Tool '{name}' has no handler"
```

#### Step 7.3: Test Agent Loop with Mocked LLM

```python
class TestAgentExecutor:
    """Test the agent loop with mocked OpenAI calls."""

    @patch("voice.agent.executor._client")
    @patch("voice.agent.executor._get_redis")
    def test_simple_text_response(self, mock_redis, mock_client):
        """Agent returns text when no tools are needed."""
        from voice.agent.executor import AgentExecutor

        mock_redis.return_value = None

        # Mock GPT-4o returning a simple text response (no tool calls)
        mock_msg = MagicMock()
        mock_msg.tool_calls = None
        mock_msg.content = "Hello! How can I help you with your coffee today?"
        mock_choice = MagicMock()
        mock_choice.message = mock_msg
        mock_response = MagicMock()
        mock_response.choices = [mock_choice]
        mock_response.usage.total_tokens = 150

        mock_client.chat.completions.create.return_value = mock_response

        executor = AgentExecutor()
        result = executor.run(transcript="Hello", user_id=1)

        assert "Hello" in result.response or "help" in result.response
        assert result.performed_write is False
        assert len(result.tool_calls) == 0

    @patch("voice.agent.executor._client")
    @patch("voice.agent.executor._get_redis")
    def test_tool_call_flow(self, mock_redis, mock_client):
        """Agent calls a tool and then responds."""
        from voice.agent.executor import AgentExecutor

        mock_redis.return_value = None

        # Turn 1: Model wants to call record_commission
        mock_tc = MagicMock()
        mock_tc.id = "call_001"
        mock_tc.function.name = "record_commission"
        mock_tc.function.arguments = json.dumps({"quantity_kg": 50, "origin": "Gedeo"})

        mock_msg1 = MagicMock()
        mock_msg1.tool_calls = [mock_tc]
        mock_msg1.content = ""

        # Turn 2: Model responds with confirmation
        mock_msg2 = MagicMock()
        mock_msg2.tool_calls = None
        mock_msg2.content = "âœ… Batch created: 50 kg from Gedeo"

        mock_client.chat.completions.create.side_effect = [
            MagicMock(choices=[MagicMock(message=mock_msg1)], usage=MagicMock(total_tokens=200)),
            MagicMock(choices=[MagicMock(message=mock_msg2)], usage=MagicMock(total_tokens=250)),
        ]

        with patch.object(AgentExecutor, "_execute_tool", return_value={
            "success": True,
            "message": "Batch created",
            "data": {"batch_id": "TEST-001"},
        }):
            executor = AgentExecutor()
            result = executor.run(transcript="Record 50 kg from Gedeo", user_id=1)

        assert "Batch created" in result.response or "50 kg" in result.response
        assert result.performed_write is True
        assert len(result.tool_calls) == 1
        assert result.tool_calls[0].tool_name == "record_commission"
```

#### Step 7.4: Test Read vs Write Classification

```python
class TestReadWriteClassification:
    """Verify read-only tools don't trigger performed_write."""

    def test_read_vs_write_classification(self):
        read_tools = {
            "query_batches", "search_knowledge",
            "browse_rfqs", "list_my_offers",
            "check_eudr_compliance", "check_mass_balance",
            "get_dpp", "get_container_dpp", "trace_lineage", "validate_dpp",
            "list_pending_verifications",
            "check_blockchain_anchor", "get_token_info", "verify_batch_hash",
        }
        write_tools = {
            "record_commission", "record_shipment", "record_receipt",
            "record_transformation", "pack_batches", "unpack_batches",
            "split_batch", "create_rfq", "submit_offer", "accept_offer",
            "verify_batch",
        }
        from voice.agent.tools import SUPPLY_CHAIN_TOOLS
        all_names = {t["function"]["name"] for t in SUPPLY_CHAIN_TOOLS}
        assert read_tools | write_tools == all_names, "Unclassified tools exist!"
```

**Run all tests:**

```bash
python -m pytest tests/test_agent.py -v

# Expected output:
# tests/test_agent.py::TestToolDefinitions::test_all_tools_have_valid_schema PASSED
# tests/test_agent.py::TestToolDefinitions::test_total_tool_count PASSED
# tests/test_agent.py::TestToolDefinitions::test_tool_names_are_unique PASSED
# tests/test_agent.py::TestToolDefinitions::test_required_fields_are_in_properties PASSED
# tests/test_agent.py::TestToolRegistry::test_all_tools_registered PASSED
# tests/test_agent.py::TestAgentExecutor::test_simple_text_response PASSED
# tests/test_agent.py::TestAgentExecutor::test_tool_call_flow PASSED
# tests/test_agent.py::TestReadWriteClassification::test_read_vs_write_classification PASSED
# ...
# ======================== 56 passed in 0.83s ========================
```

---

### Step 8: Integration â€” Connecting the Agent to the Voice Pipeline

The agent replaces the old NLU â†’ switch/case path. Here's how to integrate it:

```python
# In your Telegram bot handler or voice API endpoint:

from voice.agent import AgentExecutor

# Create executor (singleton in production)
executor = AgentExecutor()

# When a voice message comes in:
async def handle_voice_message(audio_bytes, user_id, language="en"):
    # Step 1: Transcribe (same as before)
    transcript = await transcribe(audio_bytes, language)

    # Step 2: Run agent (NEW â€” replaces old NLU pipeline)
    result = executor.run(
        transcript=transcript,
        user_id=user_id,
        language=language,
    )

    # Step 3: Respond
    if result.response_spoken:
        await send_tts(result.response_spoken, language)  # Voice response
    await send_text(result.response)                       # Text response
```

**Before (Old Pipeline):**
```python
# ~500 lines of NLU + switch/case + clarification logic
intent = extract_intent(transcript)
if intent == "record_commission":
    result = process_commission(...)
elif intent == "record_shipment":
    result = process_shipment(...)
elif intent == "unknown":
    result = "Sorry, I don't understand"
```

**After (Agent):**
```python
# 3 lines
result = executor.run(transcript=transcript, user_id=user_id, language=language)
```

---

## ğŸ§ª Exercises

1. **Add a new tool:** Define a `check_weather` tool that accepts a `location` parameter and returns weather data. Add the schema to `tools.py`, the handler to `registry.py`, and a test to `test_agent.py`. How many files did you change? (Answer: 3 files, ~30 lines. Compare to the old pipeline which would require NLU prompt changes + intent handler + validation = ~5 files, ~100 lines.)

2. **Test compound commands:** Write a test where GPT-4o calls two tools in one turn (e.g., `record_commission` + `record_shipment`). Verify both tool calls are recorded and `performed_write` is True.

3. **Add a new domain:** Create a "Quality Control" agent with two tools: `record_cupping_score` (write) and `get_quality_report` (read). Follow the full pattern: schema â†’ handler â†’ register â†’ test.

4. **Benchmark translation:** Measure the latency of the Addis AI â†’ GPT-4o-mini â†’ original cascade. How long does each level take? What percentage of requests hit each fallback level in production?

5. **Token optimization:** Calculate the exact token count of all 25 tool schemas. What's the break-even point (number of tools) where a router + specialist pattern becomes cheaper than a flat list?

---

## ğŸ‰ Lab 28 Complete Summary

### What We Built

**Four Core Modules:**

1. âœ… **Tool Definitions** (`voice/agent/tools.py` â€” 945 lines)
   - 25 OpenAI function-calling schemas
   - 7 agent domains: Core, Marketplace, Compliance, DPP, Verification, Blockchain
   - Precise descriptions for reliable tool selection
   - Type-safe JSON Schema parameters

2. âœ… **Tool Registry** (`voice/agent/registry.py` â€” 1,210 lines)
   - Maps 25 tool names â†’ Python handler functions
   - Singleton pattern for expensive resources (BlockchainAnchor, TokenManager)
   - Thin wrappers around existing Voice Ledger modules
   - Uniform handler signature: `(db, args, user_id, user_did) â†’ (message, data)`

3. âœ… **Agent Executor** (`voice/agent/executor.py` â€” 690 lines)
   - Bounded agent loop (max 6 turns)
   - GPT-4o function-calling with `tool_choice="auto"`
   - Bilingual support via translate-bracket-translate (Addis AI â†’ GPT-4o-mini â†’ original)
   - Redis-backed conversation history (10-minute TTL)
   - Read vs write classification for safety
   - TTS-safe response stripping

4. âœ… **Test Suite** (`tests/test_agent.py` â€” 1,011 lines)
   - 56 tests across 12 test classes
   - Mocked LLM calls for deterministic testing
   - Schema validation, registry completeness, read/write classification
   - Integration tests with mocked tool execution

### Architecture at a Glance

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     tools.py (schemas)    â”‚
                         â”‚  25 function definitions  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ passed to GPT-4o
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Amharic  â”‚â”€â”€â”€â–¶â”‚       executor.py (agent loop)   â”‚â”€â”€â”€â–¶â”‚   Amharic    â”‚
â”‚ STT      â”‚    â”‚  translate â†’ reason â†’ tools â†’    â”‚    â”‚   TTS        â”‚
â”‚          â”‚    â”‚  results â†’ translate â†’ respond   â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ calls handlers
                                  â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   registry.py (handlers)  â”‚
                         â”‚  25 handler functions     â”‚
                         â”‚  wraps: Labs 1-27 modules â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Numbers

| Metric | Value |
|--------|-------|
| Total tools | 25 |
| Agent domains | 7 |
| Read-only tools | 14 |
| Write tools | 11 |
| Test count | 56 |
| Test pass rate | 100% |
| Lines of code | 3,880 |
| Avg latency | 2-4 seconds |
| Cost per command | ~$0.013 |
| Max turns | 6 |

---

**Lab Status:** âœ… COMPLETE  
**Previous Lab:** Lab 27 â€” Full-Stack Integration  
**Next Lab:** Lab 29 â€” Chainlink CRE Agent-Oracle Bridge (extends the 25-tool agent to 28 tools with DON attestation)  
**What's Next:** In Lab 29, you'll bridge the agent to Chainlink's Compute Runtime Environment (CRE), adding 3 oracle tools, an auto-attestation hook, and DON-verified provenance data in Digital Product Passports. Beyond that: deploy and monitor the agent in production, add prompt caching, usage analytics, and A/B testing against the old NLU pipeline.

---
