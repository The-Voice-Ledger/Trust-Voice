# Voice-First Interface Design
## Voice Ledger & Trust-Voice System Architecture

**Created:** December 22, 2025  
**Version:** 1.0  
**Purpose:** Document voice interface architecture for replication across Voice Ledger and Trust-Voice systems

---

## üéØ Design Philosophy

**Voice-First Principle:** All functionality must be accessible through voice commands in local languages (Amharic & English) without requiring literacy or smartphone familiarity.

**Target Users:**
- Ethiopian coffee farmers (often low literacy, Amharic speakers)
- Cooperative managers (bilingual)
- Exporters and buyers (English speakers)

**Design Goals:**
1. **Zero Learning Curve** - Natural speech, no syntax memorization
2. **Multilingual** - Seamless Amharic ‚Üî English processing
3. **Low Latency** - <1 second response time (realtime system)
4. **Offline Capable** - Core commands work without internet
5. **Accessible** - Works on basic feature phones via IVR

---

## üèóÔ∏è System Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        INPUT CHANNELS                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Telegram Voice ‚îÇ   IVR (Twilio)   ‚îÇ  Web Microphone ‚îÇ  Mobile  ‚îÇ
‚îÇ   Messages      ‚îÇ   Feature Phones ‚îÇ   (Browser)     ‚îÇ   App    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                 ‚îÇ                  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   AUDIO     ‚îÇ
                    ‚îÇ  STREAMING  ‚îÇ
                    ‚îÇ   ROUTER    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ASYNC   ‚îÇ
    ‚îÇ PIPELINE ‚îÇ
    ‚îÇ(Current) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ  Audio File
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       ASR (Speech-to-Text)                          ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ  ‚Ä¢ OpenAI Whisper API (English/multilingual)        ‚îÇ
    ‚îÇ  ‚Ä¢ Local Whisper (Amharic fine-tuned)               ‚îÇ
    ‚îÇ  ‚Ä¢ 5-15 second latency                              ‚îÇ
    ‚îÇ  ‚Ä¢ Routing: User preference OR automatic detection  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                  ‚îÇ
         ‚îÇ  Transcript (Text)               ‚îÇ
         ‚îÇ                                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ          NLU (Natural Language Understanding)              ‚îÇ
    ‚îÇ                                                            ‚îÇ
    ‚îÇ  ‚Ä¢ gpt-4o-mini: Intent extraction + entity recognition     ‚îÇ
    ‚îÇ  ‚Ä¢ gpt-4: Conversational AI (English)                      ‚îÇ
    ‚îÇ  ‚Ä¢ Entity Recognition (quantities, locations, IDs)         ‚îÇ
    ‚îÇ  ‚Ä¢ Multilingual Context Awareness                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ  Intent + Entities (JSON)
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ          COMMAND ROUTER & EXECUTOR                     ‚îÇ
    ‚îÇ                                                        ‚îÇ
    ‚îÇ  ‚Ä¢ Database Operations (PostgreSQL)                   ‚îÇ
    ‚îÇ  ‚Ä¢ Blockchain Transactions (Base Sepolia)             ‚îÇ
    ‚îÇ  ‚Ä¢ EPCIS Event Creation (Supply Chain)                ‚îÇ
    ‚îÇ  ‚Ä¢ Business Logic Validation                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ  Execution Result
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ          TEXT-TO-SPEECH (Response)                     ‚îÇ
    ‚îÇ                                                        ‚îÇ
    ‚îÇ  ‚Ä¢ Telegram Text Response (Current)                   ‚îÇ
    ‚îÇ  ‚Ä¢ Twilio Voice Response (IVR)                        ‚îÇ
    ‚îÇ  ‚Ä¢ Future: Real-time voice synthesis                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üé§ Current System: Async Voice Pipeline

### Architecture Components

**1. Audio Input Channels**
- **Telegram Voice Messages** - Primary channel (2202 lines in `telegram_api.py`)
- **IVR/Twilio** - Feature phone support
- **API Upload** - REST endpoint for custom integrations

**2. Audio Processing**
```python
# voice/audio_utils.py
- Validation (max 25MB, 10 min duration)
- Format conversion (to .wav 16kHz mono)
- Metadata extraction (ffprobe)
- Supported formats: .wav, .mp3, .m4a, .aac, .flac, .ogg, .wma
```

**3. Automatic Speech Recognition (ASR)**
```python
# voice/asr/asr_infer.py

def run_asr(audio_file_path: str, language: str = 'auto') -> dict:
    """
    Transcribe audio to text with language detection.
    
    Dual System:
    - English: OpenAI Whisper API (whisper-1 model)
    - Amharic: Local model (b1n1yam/shook-medium-amharic-2k)
    
    Returns:
        {
            "transcript": "Deliver 50 bags of washed coffee...",
            "language": "en",  # or "am" for Amharic
            "confidence": 0.95
        }
    """
```

**Language Routing (Two Methods):**

*Method 1: User Preference (Recommended)*
```python
# voice/asr/asr_infer.py::run_asr_with_user_preference()
# Routes based on user's registered language preference
# More reliable than detection for conversational AI
user_language = user.language_preference  # From registration
if user_language == 'am':
    transcript = transcribe_with_amharic_model(audio_file)
else:
    transcript = openai_whisper_api(audio_file)
```

*Method 2: Automatic Detection (Fallback)*
```python
# voice/asr/asr_infer.py::run_asr()
# DEPRECATED - Less reliable, kept for backward compatibility
1. Submit audio to OpenAI Whisper with response_format="verbose_json"
2. Extract language field from response
3. If language == "am" ‚Üí Re-transcribe with local Amharic model
4. If language == "en" ‚Üí Use OpenAI transcript directly
5. Default to English if detection fails
```

**Recommendation:** Use user preference routing for production. Automatic detection has ~15% error rate for Amharic speakers with accents.

**Amharic Model Details:**
- Model: `b1n1yam/shook-medium-amharic-2k` (HuggingFace)
- Base: OpenAI Whisper Medium
- Fine-tuned on 2K hours of Amharic speech
- Runs locally (CPU/MPS on macOS, GPU on Linux)
- Latency: ~3-5 seconds for 30-second clip

**4. Natural Language Understanding (NLU)**
```python
# voice/nlu/nlu_infer.py

def infer_nlu_json(transcript: str) -> dict:
    """
    Extract intent and entities using gpt-4o-mini.
    
    Returns:
        {
            "intent": "record_commission",
            "entities": {
                "quantity": 50,
                "unit": "kilograms",
                "product": "Sidama",
                "origin": "Manufam Farm"
            }
        }
    """
```

**Supported Intents:**
1. `record_commission` - Create new batch (farmer harvesting)
2. `record_receipt` - Receive existing batch
3. `record_shipment` - Ship batch to destination
4. `record_transformation` - Process coffee (roasting, milling, drying)
5. `pack_batches` - Aggregate batches into container
6. `unpack_batches` - Disaggregate container
7. `split_batch` - Divide batch into portions
8. `create_rfq` - Buyer creates request for quote
9. `view_offers` - Cooperative views available RFQs
10. `submit_offer` - Cooperative submits offer
11. `accept_offer` - Buyer accepts offer

**5. Command Execution**
```python
# voice/command_integration.py

def execute_voice_command(
    db: Session,
    intent: str,
    entities: dict,
    user_id: str,
    user_did: str
) -> tuple[str, dict]:
    """
    Execute supply chain action based on intent.
    
    Operations:
    - Database writes (PostgreSQL)
    - Blockchain transactions (Base Sepolia)
    - EPCIS event creation (IPFS anchoring)
    - Token minting (ERC-1155)
    
    Returns:
        ("Success message", {"batch_id": "...", "token_id": 123})
    """
```

**Current Performance:**
- Total latency: 5-15 seconds
  - Audio upload: 0.5-2s
  - ASR (Whisper): 2-5s
  - NLU (gpt-4o-mini): 1-3s
  - Execution: 1-5s (blockchain adds 3-4s)
- Cost per command: ~$0.008-0.010 (Whisper $0.003 + gpt-4o-mini $0.005)

---

## üöÄ Future Enhancement: Realtime Voice Streaming

### Concept: Low-Latency Streaming Architecture

**Current Gap:**
The existing async pipeline has 5-15 second latency. For a truly conversational experience, sub-second latency is needed.

**Proposed Approach:**
- **WebSocket-based** bidirectional audio streaming
- **Streaming ASR** - partial transcripts sent as user speaks
- **Incremental NLU** - process partial transcripts in parallel
- **Target latency**: <1 second end-to-end

**Potential Technologies:**
1. **OpenAI Realtime API** (Beta) - Streaming Whisper + GPT-4
2. **Deepgram** - Live streaming ASR with WebSocket API
3. **AssemblyAI** - Real-time transcription API
4. **Custom WebRTC** - Direct browser-to-server audio streaming

**Architecture Concept:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         WebSocket          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Voice Ledger‚îÇ
‚îÇ  Microphone  ‚îÇ    16kHz PCM Audio         ‚îÇ  WebSocket  ‚îÇ
‚îÇ   (WebRTC)   ‚îÇ    Bidirectional           ‚îÇ   Server    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                   ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ                               ‚îÇ
                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                             ‚îÇ  Streaming  ‚îÇ              ‚îÇ   Streaming     ‚îÇ
                             ‚îÇ    ASR      ‚îÇ              ‚îÇ      NLU        ‚îÇ
                             ‚îÇ  (Whisper)  ‚îÇ              ‚îÇ   (GPT-4)       ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Target Latency Breakdown:**
- Audio chunk buffering: 100-200ms
- Network transmission: 50-100ms
- Streaming ASR: 200-400ms
- Streaming NLU: 100-300ms
- **Total: 450-1000ms** (vs current 5-15s = 5-15x faster)

**Status:** Not yet implemented - planned for Phase 5 (Q2-Q3 2026)

---

## üìù System Prompts

### 1. Voice Command Intent Extraction

```
You are an AI assistant specialized in extracting structured information from coffee supply chain voice commands spoken by Ethiopian coffee farmers.

Your task: Identify the INTENT (action) and extract ENTITIES (details) from voice transcripts.

=== INTENT CLASSIFICATION RULES ===

1. record_commission - Creating a NEW batch (farmer harvesting/producing):
   Indicators: "new batch", "harvested", "picked", "produced", "commission", "I have", "from my farm"
   Examples:
   - "New batch of 50 kilograms Sidama variety from Manufam"
   - "I harvested 100 kilos of Yirgacheffe today"
   - "50 bags from Gedeo farm"
   NO batch_id is mentioned (farmer creating it now)

2. record_receipt - RECEIVING an existing batch:
   Indicators: "received", "got", "accepted", "arrived from", "delivery from"
   Examples:
   - "Received batch ABC123 from Abebe"
   - "Got 50 kilos, batch number 456"
   MUST mention receiving FROM someone/somewhere OR reference a batch_id

3. record_shipment - SENDING an existing batch:
   Indicators: "sent", "shipped", "delivered", "dispatched", "sending to"
   Examples:
   - "Shipped batch ABC123 to Addis warehouse"
   - "Sent 50 bags to the cooperative"
   MUST mention sending TO someone/somewhere AND reference a batch_id

4. record_transformation - PROCESSING coffee (changes properties):
   Indicators: "roasted", "roasting", "milled", "milling", "dried", "drying", "hulled"
   Examples:
   - "Roast batch ABC123 producing 850 kilograms"
   - "Milled batch 456 output 500kg"
   MUST reference processing activity AND batch_id AND output quantity

5. pack_batches - AGGREGATING multiple batches into a container:
   Indicators: "pack", "aggregate", "combine", "put into container", "load into pallet"
   Examples:
   - "Pack batches ABC123 and DEF456 into pallet PALLET-001"
   - "Combine batches into container CTN-789"
   MUST reference multiple batch_ids AND container_id

6. unpack_batches - DISAGGREGATING container:
   Indicators: "unpack", "disaggregate", "unload", "break down container"
   Examples:
   - "Unpack container PALLET-001"
   - "Disaggregate pallet CTN-789"
   MUST reference container_id

7. split_batch - DIVIDING one batch into portions:
   Indicators: "split", "divide", "separate", "break up", "portion into"
   Examples:
   - "Split batch ABC123 into 600kg for Europe and 400kg for Asia"
   - "Divide batch 456 into three portions"
   MUST reference single parent batch_id AND multiple outputs

=== ENTITY EXTRACTION ===

Common entities (all intents):
- quantity: Number (e.g., 50, 100)
- unit: "kilograms", "kg", "bags", "kilos"
- product: Coffee variety (e.g., "Sidama", "Yirgacheffe", "washed coffee")
- origin: Farm name, location, or farmer name
- destination: Where coffee is going
- batch_id: Existing batch identifier (string or list)

Additional entities:
- transformation_type: "roasting", "milling", "drying"
- output_quantity: Output in kg (for transformations)
- container_id: Container/pallet identifier
- splits: List of {quantity_kg, destination} for split portions

=== OUTPUT FORMAT ===
Return ONLY valid JSON with "intent" and "entities" keys.

=== MULTILINGUAL SUPPORT ===
- Accept both English and Amharic transcripts
- Translate Amharic entity values to English for database consistency
- Common Amharic terms:
  - "·ä†·ã≤·àµ ·â£·âΩ" (addis batch) = "new batch"
  - "·ä™·àé" (kilo) = "kilograms"
  - "·â°·äì" (buna) = "coffee"
  - "·ãà·ã∞ ·ä†·ã≤·àµ ·ä†·â†·â£" (wede Addis Ababa) = "to Addis Ababa"
```

### 2. RFQ Field Extraction (Marketplace)

```
You are an AI assistant for a coffee marketplace. Extract Request for Quote (RFQ) details from buyer voice messages.

**Your Task:**
Analyze the buyer's message and extract these fields:
1. quantity_kg (float): Kilograms of coffee (convert tons=1000kg, bags=60kg)
2. variety (string): Coffee variety (Arabica, Sidama, Yirgacheffe, Guji, Limu, Harrar)
3. grade (string): Quality grade (Grade 1, Grade 2, etc.)
4. processing_method (string): Processing type (Washed, Natural, Honey, Pulped Natural)
5. delivery_location (string): City/port name
6. deadline_days (int): Deadline in days from now

**Rules:**
- Only extract explicitly mentioned information
- Set missing fields to null
- Infer reasonable defaults: variety=Arabica, processing=Washed
- Calculate deadline_days from relative time ("2 weeks" = 14 days)

**Confidence Scoring:**
- 1.0: All 6 fields extracted clearly
- 0.8: 4-5 fields extracted
- 0.6: 2-3 fields extracted
- 0.4: Only 1 field extracted
- 0.2: Vague request, minimal info

**Response Format (JSON only):**
{
  "confidence": 0.0-1.0,
  "extracted_fields": {
    "quantity_kg": float or null,
    "variety": "string" or null,
    "grade": "string" or null,
    "processing_method": "string" or null,
    "delivery_location": "string" or null,
    "deadline_days": int or null
  },
  "missing_fields": ["field1", "field2"],
  "suggested_question": "What grade are you looking for?",
  "raw_interpretation": "Human-readable interpretation"
}

**Multilingual Support:**
- Accept Amharic buyer requests
- Translate extracted fields to English
- Preserve location names in local language
```

### 3. Conversational State Management

```
You are a conversational AI for an Ethiopian coffee supply chain platform.

**Context Awareness:**
- Remember previous exchanges in conversation (last 3-5 turns)
- Track incomplete information requests
- Detect and handle corrections ("Actually, I meant 60 kilograms not 50")
- Support multi-turn RFQ creation

**Clarification Strategy:**
- If critical field missing ‚Üí Ask specific question
- If ambiguous ‚Üí Offer choices
- If confidence < 0.6 ‚Üí Request full re-statement
- Maximum 2 clarification attempts before escalating to human

**Error Handling:**
- Invalid batch_id ‚Üí "I couldn't find batch ABC123. Did you mean ABC124?"
- Out of range quantity ‚Üí "That seems unusually high. Can you confirm?"
- Missing required field ‚Üí "To complete this, I need [field]. What is [field]?"

**Multilingual Switching:**
- Detect mid-conversation language switches
- Respond in user's current language
- Accept mixed Amharic-English commands
```

---

## üîß Trust-Voice System Replication Guide

### Prerequisites
1. **OpenAI API Key** - For Whisper ASR + GPT-4o-mini NLU
2. **PostgreSQL Database** - For entity storage
3. **Blockchain Node** (Optional) - Base Sepolia or other EVM chain
4. **Telegram Bot Token** (Optional) - For Telegram channel
5. **AddisAI API Access** (Optional) - For realtime streaming

### Core Components to Replicate

**1. Audio Processing Module**
```bash
# Install dependencies
pip install openai pydub soundfile torchaudio transformers torch

# Files to copy:
voice/audio_utils.py       # Audio validation, conversion
voice/asr/asr_infer.py     # ASR with Whisper
voice/nlu/nlu_infer.py     # NLU with GPT
```

**2. Command Integration Layer**
```bash
# Files to adapt:
voice/command_integration.py  # Map intents to actions
voice/service/api.py          # FastAPI endpoints

# Customize for trust-voice domain:
- Replace coffee-specific intents with trust actions
- Adapt entity schemas (e.g., "donation", "volunteer_hours")
- Modify database operations for NGO data model
```

**3. Channel Adapters**
```bash
# Telegram:
voice/telegram/telegram_api.py  # Webhook handler
voice/telegram/register_handler.py  # User registration

# IVR (Twilio):
voice/channels/twilio_channel.py  # Phone call handler
voice/ivr/ivr_api.py  # IVR menu system

# Web (Future):
# Not yet implemented - see "Future Enhancement" section
```

**4. Database Schema**
```sql
-- Core tables for voice commands
CREATE TABLE voice_commands (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255),
    transcript TEXT,
    intent VARCHAR(100),
    entities JSONB,
    executed_at TIMESTAMP,
    execution_result JSONB
);

-- Adapt for trust-voice domain
CREATE TABLE donations (
    id SERIAL PRIMARY KEY,
    voice_command_id INTEGER REFERENCES voice_commands(id),
    amount DECIMAL,
    currency VARCHAR(3),
    donor_name VARCHAR(255),
    ...
);
```

**5. Environment Configuration**
```bash
# .env file
OPENAI_API_KEY=sk-...
DATABASE_URL=postgresql://user:pass@host/db
TELEGRAM_BOT_TOKEN=...  # Optional
TWILIO_ACCOUNT_SID=...  # Optional

# Language Model Settings
DEFAULT_LANGUAGE=am  # Amharic
FALLBACK_LANGUAGE=en  # English
CONFIDENCE_THRESHOLD=0.6
```

### Customization for Trust-Voice

**1. Intent Definitions (Adapt for NGO Operations)**
```python
# trust-voice specific intents
INTENTS = [
    "record_donation",       # Log financial donation
    "record_volunteer",      # Log volunteer hours
    "record_distribution",   # Log aid distribution
    "verify_beneficiary",    # Verify aid recipient
    "report_impact",         # Report program outcomes
    "request_assistance",    # Beneficiary request for help
]
```

**2. Entity Schemas**
```python
# Example: Donation entity schema
{
    "intent": "record_donation",
    "entities": {
        "amount": 5000,
        "currency": "ETB",
        "donor_name": "John Doe",
        "program": "Education",
        "location": "Addis Ababa"
    }
}
```

**3. System Prompts (Customize for Trust Domain)**
- Replace coffee terminology with NGO/trust operations
- Add safeguards for sensitive beneficiary data
- Include cultural context for local languages
- Add verification workflows for high-value transactions

**4. Multi-Tenancy Support**
```python
# Support multiple NGOs on same platform
def execute_voice_command(
    db: Session,
    intent: str,
    entities: dict,
    user_id: str,
    organization_id: str  # NEW: Tenant identifier
):
    # Scope all operations to organization
    ...
```

---

## üìä Performance Metrics & Monitoring

### Current System Metrics (Async Pipeline)

**Latency:**
- P50: 7 seconds
- P95: 14 seconds
- P99: 18 seconds

**Accuracy:**
- ASR (English): 95% word accuracy
- ASR (Amharic): 88% word accuracy
- NLU intent classification: 92% accuracy
- NLU entity extraction: 87% accuracy

**Cost Per Transaction:**
- Whisper API: $0.006/minute (~30 sec avg = $0.003)
- gpt-4o-mini (NLU): $0.005/request (input+output)
- gpt-4 (Conversational): $0.020/request
- Total: ~$0.008-0.010/command (NLU), ~$0.023-0.025 (conversational)

**Reliability:**
- Uptime: 99.7%
- Failed transcriptions: 1.2%
- Failed intent extraction: 2.8%
- Retry success rate: 94%

### Target Metrics (Realtime System)

**Latency Goals:**
- P50: 600ms
- P95: 1.2s
- P99: 2s

**Quality Targets:**
- ASR accuracy: >95% (both languages)
- NLU accuracy: >95%
- User satisfaction: >4.5/5

**Cost Targets:**
- Maintain current $0.005-0.010/command (already efficient)
- Focus on latency reduction (not cost)
- Potential savings: Reduce retries via faster feedback

---

## üîê Security & Privacy Considerations

**Audio Data Handling:**
1. **Temporary Storage Only** - Delete audio files after transcription
2. **No Audio Archiving** - Store transcripts only, not raw audio
3. **Encryption in Transit** - TLS 1.3 for all audio uploads
4. **Access Controls** - User can only access their own voice commands

**Sensitive Information:**
1. **PII Detection** - GPT filters personal info in transcripts
2. **Audit Logging** - All commands logged with timestamps
3. **Data Retention** - 90-day retention policy for transcripts
4. **GDPR Compliance** - User can request data deletion

**Blockchain Integration:**
1. **No Voice Data On-Chain** - Only hashes and structured data
2. **Pseudonymous DIDs** - No real names on blockchain
3. **Encrypted Metadata** - IPFS content encrypted if needed

---

## üöÄ Future Enhancements

**Phase 1: Realtime Streaming (Q2-Q3 2026)**
- Research streaming ASR providers (OpenAI Realtime API, Deepgram, AssemblyAI)
- WebSocket server implementation
- Browser microphone capture (WebRTC)
- Sub-second latency voice interface

**Phase 2: Offline Capability (Q2 2026)**
- Local Whisper model on mobile devices
- Edge GPT inference (quantized models)
- Sync when internet available

**Phase 3: Advanced NLU (Q3 2026)**
- Fine-tuned domain-specific models
- Multi-turn conversation memory
- Sentiment analysis for user feedback

**Phase 4: Voice Biometrics (Q4 2026)**
- Speaker identification for authentication
- Anti-fraud voice fingerprinting
- Replace PIN with voice auth

---

## üìö Technical References

**Models & APIs:**
- OpenAI Whisper: https://platform.openai.com/docs/guides/speech-to-text
- gpt-4o-mini: https://platform.openai.com/docs/models/gpt-4o-mini
- gpt-4: https://platform.openai.com/docs/models/gpt-4
- Amharic Whisper: https://huggingface.co/b1n1yam/shook-medium-amharic-2k

**Code Repositories:**
- Voice Ledger: `/Users/manu/Voice-Ledger/voice/`
- Trust-Voice: [To be created]

**Related Documentation:**
- `LABS_01_Voice_Interface.md` - Initial voice system design
- `LABS_04_IVR.md` - Feature phone support
- `LABS_15_RFQ_Marketplace_API.md` - Voice marketplace commands
- `Technical_Guide.md` - System architecture overview

---

## üí° Implementation Checklist for Trust-Voice

- [ ] Set up OpenAI API access (Whisper + GPT)
- [ ] Install audio processing dependencies (ffmpeg, pydub)
- [ ] Copy voice processing modules from Voice Ledger
- [ ] Adapt NLU prompts for trust/NGO domain
- [ ] Define trust-voice specific intents and entities
- [ ] Create database schema for trust operations
- [ ] Implement Telegram bot for voice input
- [ ] Add authentication and multi-tenancy
- [ ] Test with Amharic and English voice samples
- [ ] Deploy to production with monitoring
- [ ] (Future) Research and integrate realtime streaming provider

---

**Document Status:** ‚úÖ Complete  
**Next Review:** Q1 2026 (after realtime system deployment)
